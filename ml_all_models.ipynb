{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ecd81c-a7b9-4948-8973-057460278e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, StackingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7920661-7afd-418f-94ad-6758ae961df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================================\n",
    "# SMART DATA CONVERSION\n",
    "#=============================================================================\n",
    "\n",
    "def convert_binary_columns(df):\n",
    "    \"\"\"Convert binary text values to 0/1\"\"\"\n",
    "    df_converted = df.copy()\n",
    "    conversions_made = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            unique_vals = df[col].dropna().unique()\n",
    "            unique_vals_lower = [str(v).lower().strip() for v in unique_vals]\n",
    "\n",
    "            if len(unique_vals) <= 2:\n",
    "                binary_mappings = {\n",
    "                    ('yes', 'no'): {'yes': 1, 'no': 0},\n",
    "                    ('y', 'n'): {'y': 1, 'n': 0},\n",
    "                    ('true', 'false'): {'true': 1, 'false': 0},\n",
    "                    ('t', 'f'): {'t': 1, 'f': 0},\n",
    "                    ('1', '0'): {'1': 1, '0': 0},\n",
    "                    ('1.0', '0.0'): {'1.0': 1, '0.0': 0},\n",
    "                    ('positive', 'negative'): {'positive': 1, 'negative': 0},\n",
    "                    ('pos', 'neg'): {'pos': 1, 'neg': 0},\n",
    "                    ('present', 'absent'): {'present': 1, 'absent': 0},\n",
    "                    ('active', 'inactive'): {'active': 1, 'inactive': 0},\n",
    "                }\n",
    "\n",
    "                unique_set = set(unique_vals_lower)\n",
    "                for key_pair, mapping in binary_mappings.items():\n",
    "                    if unique_set.issubset(set(key_pair)):\n",
    "                        case_insensitive_map = {}\n",
    "                        for orig_val in unique_vals:\n",
    "                            orig_lower = str(orig_val).lower().strip()\n",
    "                            if orig_lower in mapping:\n",
    "                                case_insensitive_map[orig_val] = mapping[orig_lower]\n",
    "\n",
    "                        df_converted[col] = df[col].map(case_insensitive_map)\n",
    "                        conversions_made[col] = f\"Converted to binary: {dict(case_insensitive_map)}\"\n",
    "                        break\n",
    "\n",
    "    return df_converted, conversions_made\n",
    "\n",
    "\n",
    "def smart_data_preparation(df, target_col=None):\n",
    "    \"\"\"Smart data preparation with binary conversion\"\"\"\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    if target_col and target_col in df.columns:\n",
    "        y = df[target_col]\n",
    "        df_clean = df.drop(target_col, axis=1)\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    if 'Id' in df_clean.columns:\n",
    "        df_clean = df_clean.drop('Id', axis=1)\n",
    "\n",
    "    print(f\"\\nInitial features: {list(df_clean.columns)}\")\n",
    "    print(f\"Initial shape: {df_clean.shape}\")\n",
    "\n",
    "    print(\"\\nStep 1: Converting binary text columns...\")\n",
    "    df_clean, conversions = convert_binary_columns(df_clean)\n",
    "\n",
    "    if conversions:\n",
    "        for col, conversion_info in conversions.items():\n",
    "            print(f\"  ✓ {col}: {conversion_info}\")\n",
    "    else:\n",
    "        print(\"  No binary text columns found\")\n",
    "\n",
    "    print(\"\\nStep 2: Converting to numeric types...\")\n",
    "    numeric_features = []\n",
    "    categorical_features = []\n",
    "\n",
    "    for col in df_clean.columns:\n",
    "        if df_clean[col].dtype in ['int64', 'float64']:\n",
    "            numeric_features.append(col)\n",
    "            print(f\"  ✓ {col}: Already numeric\")\n",
    "        else:\n",
    "            converted = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "            non_null_ratio = converted.notna().sum() / len(df_clean)\n",
    "\n",
    "            if non_null_ratio >= 0.9:\n",
    "                df_clean[col] = converted\n",
    "                numeric_features.append(col)\n",
    "                print(f\"  ✓ {col}: Converted to numeric ({non_null_ratio*100:.1f}% success)\")\n",
    "            elif non_null_ratio > 0:\n",
    "                categorical_features.append(col)\n",
    "                print(f\"  → {col}: Kept as categorical (only {non_null_ratio*100:.1f}% numeric)\")\n",
    "            else:\n",
    "                unique_count = df_clean[col].nunique()\n",
    "                if unique_count > 0 and unique_count < 50:\n",
    "                    categorical_features.append(col)\n",
    "                    print(f\"  → {col}: Kept as categorical ({unique_count} unique values)\")\n",
    "                else:\n",
    "                    print(f\"  ✗ {col}: Cannot use (all missing or too many categories)\")\n",
    "\n",
    "    print(\"\\nStep 3: Handling missing values...\")\n",
    "    for col in numeric_features:\n",
    "        missing_count = df_clean[col].isna().sum()\n",
    "        if missing_count > 0:\n",
    "            median_val = df_clean[col].median()\n",
    "            df_clean[col].fillna(median_val, inplace=True)\n",
    "            print(f\"  Filled {missing_count} missing in '{col}' with median: {median_val:.2f}\")\n",
    "\n",
    "    for col in categorical_features:\n",
    "        missing_count = df_clean[col].isna().sum()\n",
    "        if missing_count > 0:\n",
    "            mode_val = df_clean[col].mode()[0] if not df_clean[col].mode().empty else 'Unknown'\n",
    "            df_clean[col].fillna(mode_val, inplace=True)\n",
    "            print(f\"  Filled {missing_count} missing in '{col}'\")\n",
    "\n",
    "    print(f\"\\nFinal feature summary:\")\n",
    "    print(f\"  Numeric: ({len(numeric_features)}): {numeric_features}\")\n",
    "    print(f\"  Categorical: ({len(categorical_features)}): {categorical_features}\")\n",
    "    print(f\"  Shape: {df_clean.shape}\")\n",
    "\n",
    "    if y is not None:\n",
    "        return df_clean, y, numeric_features, categorical_features\n",
    "    else:\n",
    "        return df_clean, numeric_features, categorical_features\n",
    "\n",
    "\n",
    "def create_engineered_features(df, numeric_features):\n",
    "    \"\"\"Create interaction and polynomial features\"\"\"\n",
    "    df_new = df.copy()\n",
    "    valid_numeric = [f for f in numeric_features if f in df.columns]\n",
    "\n",
    "    if len(valid_numeric) < 2:\n",
    "        return df_new\n",
    "\n",
    "    print(\"\\nCreating engineered features:\")\n",
    "\n",
    "    interactions = 0\n",
    "    for i, col1 in enumerate(valid_numeric):\n",
    "        for col2 in valid_numeric[i+1:]:\n",
    "            try:\n",
    "                df_new[f'{col1}_x_{col2}'] = df[col1] * df[col2]\n",
    "                interactions += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    squared = 0\n",
    "    for col in valid_numeric:\n",
    "        try:\n",
    "            df_new[f'{col}_squared'] = df[col] ** 2\n",
    "            squared += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    ratios = 0\n",
    "    for i, col1 in enumerate(valid_numeric):\n",
    "        for col2 in valid_numeric[i+1:]:\n",
    "            try:\n",
    "                df_new[f'{col1}_div_{col2}'] = df[col1] / (df[col2] + 0.01)\n",
    "                ratios += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    print(f\"  Interactions: {interactions}, Squared: {squared}, Ratios: {ratios}\")\n",
    "    print(f\"  Total features: {df_new.shape[1]}\")\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5de35cd8-7d35-4135-a444-19e381ec9052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================================\n",
    "# PREPROCESSING AND MODELS\n",
    "#=============================================================================\n",
    "\n",
    "def create_preprocessing_pipeline(numeric_features, categorical_features):\n",
    "    \"\"\"Create preprocessing pipeline\"\"\"\n",
    "    transformers = []\n",
    "\n",
    "    if len(numeric_features) > 0:\n",
    "        transformers.append(('num', StandardScaler(), numeric_features))\n",
    "\n",
    "    if len(categorical_features) > 0:\n",
    "        transformers.append(('cat', \n",
    "                           OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'),\n",
    "                           categorical_features))\n",
    "\n",
    "    return ColumnTransformer(transformers=transformers, remainder='drop')\n",
    "\n",
    "\n",
    "def get_models():\n",
    "    \"\"\"\n",
    "    Model configurations WITH POLYNOMIAL LINEAR REGRESSION TUNING!\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'Linear Regression (degree=1)': {\n",
    "            'model': LinearRegression(),\n",
    "            'params': {},\n",
    "            'use_poly': False,\n",
    "            'description': 'Standard OLS - baseline'\n",
    "        },\n",
    "        'Polynomial LR (degree=2)': {\n",
    "            'model': LinearRegression(),\n",
    "            'params': {},\n",
    "            'use_poly': True,\n",
    "            'poly_degree': 2,\n",
    "            'description': 'Adds x², xy interactions'\n",
    "        },\n",
    "        'Polynomial LR (degree=3)': {\n",
    "            'model': LinearRegression(),\n",
    "            'params': {},\n",
    "            'use_poly': True,\n",
    "            'poly_degree': 3,\n",
    "            'description': 'Adds x³, x²y interactions'\n",
    "        },\n",
    "        'Ridge': {\n",
    "            'model': Ridge(random_state=RANDOM_STATE),\n",
    "            'params': {'alpha': [0.01, 0.1, 1, 10, 100]},\n",
    "            'use_poly': False\n",
    "        },\n",
    "        'Lasso': {\n",
    "            'model': Lasso(random_state=RANDOM_STATE, max_iter=5000),\n",
    "            'params': {'alpha': [0.0001, 0.001, 0.01, 0.1, 1]},\n",
    "            'use_poly': False\n",
    "        },\n",
    "        'ElasticNet': {\n",
    "            'model': ElasticNet(random_state=RANDOM_STATE, max_iter=5000),\n",
    "            'params': {'alpha': [0.001, 0.01, 0.1, 1], 'l1_ratio': [0.3, 0.5, 0.7, 0.9]},\n",
    "            'use_poly': False\n",
    "        },\n",
    "        'BayesianRidge': {\n",
    "            'model': BayesianRidge(),\n",
    "            'params': {},\n",
    "            'use_poly': False\n",
    "        },\n",
    "        'DecisionTree': {\n",
    "            'model': DecisionTreeRegressor(random_state=RANDOM_STATE),\n",
    "            'params': {'max_depth': [5, 10, 15, 20], 'min_samples_split': [2, 5, 10]},\n",
    "            'use_poly': False\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'model': XGBRegressor(random_state=RANDOM_STATE, n_jobs=-1, verbosity=0),\n",
    "            'params': {\n",
    "                'n_estimators': [300, 500, 700],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'learning_rate': [0.01, 0.05, 0.1],\n",
    "                'subsample': [0.8, 1.0],\n",
    "                'colsample_bytree': [0.8, 1.0]\n",
    "            },\n",
    "            'randomized': True,\n",
    "            'use_poly': False\n",
    "        },\n",
    "        # 'LightGBM': {\n",
    "        #     'model': LGBMRegressor(random_state=RANDOM_STATE, n_jobs=-1, verbose=-1),\n",
    "        #     'params': {\n",
    "        #         'n_estimators': [300, 500, 700],\n",
    "        #         'max_depth': [5, 7, 10],\n",
    "        #         'learning_rate': [0.01, 0.05, 0.1],\n",
    "        #         'num_leaves': [31, 63, 127]\n",
    "        #     },\n",
    "        #     'randomized': True,\n",
    "        #     'use_poly': False\n",
    "        # },\n",
    "        'AdaBoost': {\n",
    "            'model': AdaBoostRegressor(random_state=RANDOM_STATE),\n",
    "            'params': {'n_estimators': [50, 100, 200], 'learning_rate': [0.1, 0.5, 1.0]},\n",
    "            'use_poly': False\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(X, y, name, config, numeric_features, categorical_features, cv=5):\n",
    "    \"\"\"Train a single model with optional polynomial features\"\"\"\n",
    "\n",
    "    # Check if polynomial features should be added\n",
    "    use_poly = config.get('use_poly', False)\n",
    "    poly_degree = config.get('poly_degree', 2)\n",
    "\n",
    "    # Create pipeline\n",
    "    preprocessor = create_preprocessing_pipeline(numeric_features, categorical_features)\n",
    "\n",
    "    if use_poly:\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('poly', PolynomialFeatures(degree=poly_degree, include_bias=False)),\n",
    "            ('model', config['model'])\n",
    "        ])\n",
    "    else:\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', config['model'])\n",
    "        ])\n",
    "\n",
    "    params = config.get('params', {})\n",
    "    use_randomized = config.get('randomized', False)\n",
    "\n",
    "    if params:\n",
    "        param_grid = {f'model__{k}': v for k, v in params.items()}\n",
    "\n",
    "        if use_randomized:\n",
    "            search = RandomizedSearchCV(pipeline, param_grid, n_iter=20, cv=cv,\n",
    "                                       scoring='neg_root_mean_squared_error',\n",
    "                                       n_jobs=-1, random_state=RANDOM_STATE, verbose=0)\n",
    "        else:\n",
    "            search = GridSearchCV(pipeline, param_grid, cv=cv,\n",
    "                                 scoring='neg_root_mean_squared_error',\n",
    "                                 n_jobs=-1, verbose=0)\n",
    "\n",
    "        search.fit(X, y)\n",
    "        return {\n",
    "            'name': name,\n",
    "            'model': search.best_estimator_,\n",
    "            'cv_rmse': -search.best_score_,\n",
    "            'params': search.best_params_\n",
    "        }\n",
    "    else:\n",
    "        scores = cross_val_score(pipeline, X, y, cv=cv,\n",
    "                                scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "        pipeline.fit(X, y)\n",
    "        return {\n",
    "            'name': name,\n",
    "            'model': pipeline,\n",
    "            'cv_rmse': -scores.mean(),\n",
    "            'params': {}\n",
    "        }\n",
    "\n",
    "\n",
    "def train_all_models(X, y, numeric_features, categorical_features, cv=5):\n",
    "    \"\"\"Train all models\"\"\"\n",
    "    models = get_models()\n",
    "    results = []\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TRAINING MODELS (3 Linear Regression variants + 8 others)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for name, config in models.items():\n",
    "        desc = config.get('description', '')\n",
    "        desc_str = f\" ({desc})\" if desc else \"\"\n",
    "        print(f\"\\nTraining {name}{desc_str}...\", end='', flush=True)\n",
    "        try:\n",
    "            result = train_model(X, y, name, config, numeric_features, categorical_features, cv)\n",
    "            results.append(result)\n",
    "            print(f\" ✓ CV RMSE: {result['cv_rmse']:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\" ✗ Error: {str(e)}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def create_ensemble(X, y, results, numeric_features, categorical_features, cv=5):\n",
    "    \"\"\"Create stacking ensemble\"\"\"\n",
    "    if len(results) < 3:\n",
    "        return None\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CREATING STACKING ENSEMBLE\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    sorted_results = sorted(results, key=lambda x: x['cv_rmse'])[:3]\n",
    "    print(f\"\\nTop 3 models:\")\n",
    "    for r in sorted_results:\n",
    "        print(f\"  - {r['name']}: {r['cv_rmse']:.4f}\")\n",
    "\n",
    "    preprocessor = create_preprocessing_pipeline(numeric_features, categorical_features)\n",
    "    estimators = [(r['name'], r['model'].named_steps['model'] if 'model' in r['model'].named_steps else r['model'].named_steps['poly']) for r in sorted_results]\n",
    "\n",
    "    stacking = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('stacker', StackingRegressor(estimators=estimators,\n",
    "                                     final_estimator=Ridge(alpha=1.0),\n",
    "                                     cv=cv, n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "    scores = cross_val_score(stacking, X, y, cv=cv,\n",
    "                            scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "    cv_rmse = -scores.mean()\n",
    "\n",
    "    print(f\"\\nStacking Ensemble CV RMSE: {cv_rmse:.4f}\")\n",
    "\n",
    "    stacking.fit(X, y)\n",
    "\n",
    "    return {'name': 'Stacking Ensemble', 'model': stacking, 'cv_rmse': cv_rmse, 'params': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50c916ce-0299-45e3-952b-726c31743575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created output folder: predictions_all_models\n",
      "================================================================================\n",
      "SMART PIPELINE - Generating Predictions for ALL Models\n",
      "================================================================================\n",
      "\n",
      "[Step 1/5] Loading data...\n",
      "\n",
      "Initial features: ['Therapy Hours', 'Initial Health Score', 'Lifestyle Activities', 'Average Sleep Hours', 'Follow-Up Sessions']\n",
      "Initial shape: (8000, 5)\n",
      "\n",
      "Step 1: Converting binary text columns...\n",
      "  ✓ Lifestyle Activities: Converted to binary: {'No': 0, 'Yes': 1}\n",
      "\n",
      "Step 2: Converting to numeric types...\n",
      "  ✓ Therapy Hours: Already numeric\n",
      "  ✓ Initial Health Score: Already numeric\n",
      "  ✓ Lifestyle Activities: Already numeric\n",
      "  ✓ Average Sleep Hours: Already numeric\n",
      "  ✓ Follow-Up Sessions: Already numeric\n",
      "\n",
      "Step 3: Handling missing values...\n",
      "\n",
      "Final feature summary:\n",
      "  Numeric: (5): ['Therapy Hours', 'Initial Health Score', 'Lifestyle Activities', 'Average Sleep Hours', 'Follow-Up Sessions']\n",
      "  Categorical: (0): []\n",
      "  Shape: (8000, 5)\n",
      "\n",
      "Target: Mean=55.31, Std=19.20, Range=[10.00, 100.00]\n",
      "\n",
      "[Step 2/5] Creating engineered features...\n",
      "\n",
      "Creating engineered features:\n",
      "  Interactions: 10, Squared: 5, Ratios: 10\n",
      "  Total features: 30\n",
      "\n",
      "[Step 3/5] Training models...\n",
      "\n",
      "================================================================================\n",
      "TRAINING MODELS (3 Linear Regression variants + 8 others)\n",
      "================================================================================\n",
      "\n",
      "Training Linear Regression (degree=1) (Standard OLS - baseline)... ✓ CV RMSE: 2.0501\n",
      "\n",
      "Training Polynomial LR (degree=2) (Adds x², xy interactions)... ✓ CV RMSE: 2.0829\n",
      "\n",
      "Training Polynomial LR (degree=3) (Adds x³, x²y interactions)... ✓ CV RMSE: 2.2220\n",
      "\n",
      "Training Ridge... ✓ CV RMSE: 2.0500\n",
      "\n",
      "Training Lasso... ✓ CV RMSE: 2.0478\n",
      "\n",
      "Training ElasticNet..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/mlproj/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.237e+03, tolerance: 2.361e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/mlproj/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.727e+03, tolerance: 2.356e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/mlproj/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.750e+03, tolerance: 2.356e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/mlproj/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.121e+02, tolerance: 2.356e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/mlproj/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.648e+02, tolerance: 2.356e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/mlproj/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.469e+02, tolerance: 2.365e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/mlproj/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.585e+03, tolerance: 2.365e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/mlproj/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.412e+02, tolerance: 2.361e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ CV RMSE: 2.0491\n",
      "\n",
      "Training BayesianRidge... ✓ CV RMSE: 2.0500\n",
      "\n",
      "Training DecisionTree... ✓ CV RMSE: 2.5973\n",
      "\n",
      "Training XGBoost... ✓ CV RMSE: 2.1246\n",
      "\n",
      "Training AdaBoost... ✓ CV RMSE: 2.5597\n",
      "\n",
      "[Step 4/5] Creating ensemble...\n",
      "\n",
      "================================================================================\n",
      "CREATING STACKING ENSEMBLE\n",
      "================================================================================\n",
      "\n",
      "Top 3 models:\n",
      "  - Lasso: 2.0478\n",
      "  - ElasticNet: 2.0491\n",
      "  - Ridge: 2.0500\n",
      "\n",
      "Stacking Ensemble CV RMSE: 2.0483\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "\n",
      "                       Model  CV RMSE\n",
      "                       Lasso 2.047846\n",
      "           Stacking Ensemble 2.048283\n",
      "                  ElasticNet 2.049051\n",
      "                       Ridge 2.050025\n",
      "               BayesianRidge 2.050036\n",
      "Linear Regression (degree=1) 2.050057\n",
      "    Polynomial LR (degree=2) 2.082887\n",
      "                     XGBoost 2.124644\n",
      "    Polynomial LR (degree=3) 2.222039\n",
      "                    AdaBoost 2.559723\n",
      "                DecisionTree 2.597303\n",
      "\n",
      "Best Model: Lasso\n",
      "Best CV RMSE: 2.0478\n",
      "\n",
      "📊 Linear Regression Comparison:\n",
      "   Linear Regression (degree=1): 2.0501\n",
      "   Polynomial LR (degree=2): 2.0829\n",
      "   Polynomial LR (degree=3): 2.2220\n",
      "\n",
      "[Step 5/5] Generating predictions for ALL models...\n",
      "\n",
      "Initial features: ['Therapy Hours', 'Initial Health Score', 'Lifestyle Activities', 'Average Sleep Hours', 'Follow-Up Sessions']\n",
      "Initial shape: (2000, 5)\n",
      "\n",
      "Step 1: Converting binary text columns...\n",
      "  ✓ Lifestyle Activities: Converted to binary: {'No': 0, 'Yes': 1}\n",
      "\n",
      "Step 2: Converting to numeric types...\n",
      "  ✓ Therapy Hours: Already numeric\n",
      "  ✓ Initial Health Score: Already numeric\n",
      "  ✓ Lifestyle Activities: Already numeric\n",
      "  ✓ Average Sleep Hours: Already numeric\n",
      "  ✓ Follow-Up Sessions: Already numeric\n",
      "\n",
      "Step 3: Handling missing values...\n",
      "\n",
      "Final feature summary:\n",
      "  Numeric: (5): ['Therapy Hours', 'Initial Health Score', 'Lifestyle Activities', 'Average Sleep Hours', 'Follow-Up Sessions']\n",
      "  Categorical: (0): []\n",
      "  Shape: (2000, 5)\n",
      "\n",
      "Creating engineered features:\n",
      "  Interactions: 10, Squared: 5, Ratios: 10\n",
      "  Total features: 30\n",
      "\n",
      "================================================================================\n",
      "GENERATING SEPARATE PREDICTIONS FOR EACH MODEL\n",
      "================================================================================\n",
      "\n",
      "1/11 Generating predictions for: Linear Regression (degree=1)... ✓\n",
      "      Saved: predictions_all_models/Linear_Regression_degree1.csv\n",
      "      Mean=54.96, Std=19.02, Range=[12.43, 97.65]\n",
      "\n",
      "2/11 Generating predictions for: Polynomial LR (degree=2)... ✓\n",
      "      Saved: predictions_all_models/Polynomial_LR_degree2.csv\n",
      "      Mean=54.95, Std=19.05, Range=[11.80, 98.26]\n",
      "\n",
      "3/11 Generating predictions for: Polynomial LR (degree=3)... ✓\n",
      "      Saved: predictions_all_models/Polynomial_LR_degree3.csv\n",
      "      Mean=54.96, Std=19.09, Range=[10.70, 98.97]\n",
      "\n",
      "4/11 Generating predictions for: Ridge... ✓\n",
      "      Saved: predictions_all_models/Ridge.csv\n",
      "      Mean=54.96, Std=19.02, Range=[12.44, 97.66]\n",
      "\n",
      "5/11 Generating predictions for: Lasso... ✓\n",
      "      Saved: predictions_all_models/Lasso.csv\n",
      "      Mean=54.96, Std=19.01, Range=[12.77, 97.81]\n",
      "\n",
      "6/11 Generating predictions for: ElasticNet... ✓\n",
      "      Saved: predictions_all_models/ElasticNet.csv\n",
      "      Mean=54.96, Std=19.02, Range=[12.67, 97.77]\n",
      "\n",
      "7/11 Generating predictions for: BayesianRidge... ✓\n",
      "      Saved: predictions_all_models/BayesianRidge.csv\n",
      "      Mean=54.96, Std=19.02, Range=[12.45, 97.68]\n",
      "\n",
      "8/11 Generating predictions for: DecisionTree... ✓\n",
      "      Saved: predictions_all_models/DecisionTree.csv\n",
      "      Mean=54.96, Std=19.08, Range=[12.64, 99.67]\n",
      "\n",
      "9/11 Generating predictions for: XGBoost... ✓\n",
      "      Saved: predictions_all_models/XGBoost.csv\n",
      "      Mean=54.96, Std=19.00, Range=[13.37, 97.83]\n",
      "\n",
      "10/11 Generating predictions for: AdaBoost... ✓\n",
      "      Saved: predictions_all_models/AdaBoost.csv\n",
      "      Mean=55.11, Std=18.44, Range=[16.94, 92.74]\n",
      "\n",
      "11/11 Generating predictions for: Stacking Ensemble... ✓\n",
      "      Saved: predictions_all_models/Stacking_Ensemble.csv\n",
      "      Mean=54.96, Std=19.02, Range=[12.76, 97.86]\n",
      "\n",
      "================================================================================\n",
      "BONUS: Creating Averaged Prediction from Top 3 Models\n",
      "================================================================================\n",
      "\n",
      "Averaging predictions from:\n",
      "  1. Lasso (CV RMSE: 2.0478)\n",
      "  2. Stacking Ensemble (CV RMSE: 2.0483)\n",
      "  3. ElasticNet (CV RMSE: 2.0491)\n",
      "\n",
      "✓ Saved averaged predictions: predictions_all_models/TOP3_AVERAGED.csv\n",
      "  Mean=54.96, Std=19.01\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Generated 11 prediction files\n",
      "All files saved in: predictions_all_models/\n",
      "\n",
      "Files created:\n",
      "    1. Linear_Regression_degree1.csv - Linear Regression (degree=1)\n",
      "    2. Polynomial_LR_degree2.csv - Polynomial LR (degree=2)\n",
      "    3. Polynomial_LR_degree3.csv - Polynomial LR (degree=3)\n",
      "    4. Ridge.csv - Ridge\n",
      "    5. Lasso.csv - Lasso\n",
      "    6. ElasticNet.csv - ElasticNet\n",
      "    7. BayesianRidge.csv - BayesianRidge\n",
      "    8. DecisionTree.csv - DecisionTree\n",
      "    9. XGBoost.csv - XGBoost\n",
      "   10. AdaBoost.csv - AdaBoost\n",
      "   11. Stacking_Ensemble.csv - Stacking Ensemble\n",
      "   12. TOP3_AVERAGED.csv - Average of top 3 models\n",
      "\n",
      "RECOMMENDATION:\n",
      "   - Start by submitting: predictions_all_models/TOP3_AVERAGED.csv\n",
      "   - Then try: Lasso.csv\n",
      "   - Compare scores and pick the best!\n",
      "\n",
      "================================================================================\n",
      "COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#=============================================================================\n",
    "# MODIFIED MAIN PIPELINE - GENERATES PREDICTIONS FOR ALL MODELS\n",
    "#=============================================================================\n",
    "\n",
    "def run_pipeline_all_models(train_path, test_path=None, output_folder='predictions_all_models',\n",
    "                            engineer_features=True, use_ensemble=True):\n",
    "    \"\"\"Run complete pipeline and generate separate predictions for EACH model\"\"\"\n",
    "    import os\n",
    "\n",
    "    # Create output folder if it doesn't exist\n",
    "    if test_path and not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"\\nCreated output folder: {output_folder}\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"SMART PIPELINE - Generating Predictions for ALL Models\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"\\n[Step 1/5] Loading data...\")\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    X, y, numeric_features, categorical_features = smart_data_preparation(\n",
    "        train_df, target_col='Recovery Index'\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTarget: Mean={y.mean():.2f}, Std={y.std():.2f}, Range=[{y.min():.2f}, {y.max():.2f}]\")\n",
    "\n",
    "    if engineer_features and len(numeric_features) >= 2:\n",
    "        print(\"\\n[Step 2/5] Creating engineered features...\")\n",
    "        X_cols_before = list(X.columns)\n",
    "        X = create_engineered_features(X, numeric_features)\n",
    "        new_features = [c for c in X.columns if c not in X_cols_before]\n",
    "        numeric_features = numeric_features + new_features\n",
    "    else:\n",
    "        print(\"\\n[Step 2/5] Skipping feature engineering\")\n",
    "\n",
    "    print(\"\\n[Step 3/5] Training models...\")\n",
    "    results = train_all_models(X, y, numeric_features, categorical_features, cv=5)\n",
    "\n",
    "    if use_ensemble and len(results) >= 3:\n",
    "        print(\"\\n[Step 4/5] Creating ensemble...\")\n",
    "        ensemble = create_ensemble(X, y, results, numeric_features, categorical_features, cv=5)\n",
    "        if ensemble:\n",
    "            results.append(ensemble)\n",
    "    else:\n",
    "        print(\"\\n[Step 4/5] Skipping ensemble\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    results_df = pd.DataFrame([\n",
    "        {'Model': r['name'], 'CV RMSE': r['cv_rmse']}\n",
    "        for r in results\n",
    "    ]).sort_values('CV RMSE')\n",
    "\n",
    "    print(\"\\n\" + results_df.to_string(index=False))\n",
    "\n",
    "    best = min(results, key=lambda x: x['cv_rmse'])\n",
    "    print(f\"\\nBest Model: {best['name']}\")\n",
    "    print(f\"Best CV RMSE: {best['cv_rmse']:.4f}\")\n",
    "\n",
    "    # Compare Linear Regression variants\n",
    "    linear_results = [r for r in results if 'Linear' in r['name'] or 'Polynomial LR' in r['name']]\n",
    "    if linear_results:\n",
    "        print(\"\\n📊 Linear Regression Comparison:\")\n",
    "        for r in sorted(linear_results, key=lambda x: x['cv_rmse']):\n",
    "            print(f\"   {r['name']}: {r['cv_rmse']:.4f}\")\n",
    "\n",
    "    if test_path:\n",
    "        print(\"\\n[Step 5/5] Generating predictions for ALL models...\")\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        test_ids = test_df['Id'].copy() if 'Id' in test_df.columns else None\n",
    "\n",
    "        X_test, test_num, test_cat = smart_data_preparation(test_df)\n",
    "\n",
    "        if engineer_features and len(numeric_features) >= 2:\n",
    "            X_test = create_engineered_features(X_test, test_num)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"GENERATING SEPARATE PREDICTIONS FOR EACH MODEL\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # Generate predictions for EACH model\n",
    "        all_predictions = {}\n",
    "\n",
    "        for i, result in enumerate(results, 1):\n",
    "            model_name = result['name']\n",
    "            model = result['model']\n",
    "\n",
    "            # Create safe filename (remove special characters)\n",
    "            safe_name = model_name.replace(' ', '_').replace('(', '').replace(')', '').replace('=', '')\n",
    "            filename = f\"{output_folder}/{safe_name}.csv\"\n",
    "\n",
    "            print(f\"\\n{i}/{len(results)} Generating predictions for: {model_name}...\", end='', flush=True)\n",
    "\n",
    "            try:\n",
    "                # Make predictions\n",
    "                predictions = model.predict(X_test)\n",
    "\n",
    "                # Store predictions\n",
    "                all_predictions[model_name] = predictions\n",
    "\n",
    "                # Save to CSV\n",
    "                submission = pd.DataFrame({\n",
    "                    'Id': test_ids if test_ids is not None else range(len(predictions)),\n",
    "                    'Recovery Index': predictions\n",
    "                })\n",
    "                submission.to_csv(filename, index=False)\n",
    "\n",
    "                print(f\" ✓\")\n",
    "                print(f\"      Saved: {filename}\")\n",
    "                print(f\"      Mean={predictions.mean():.2f}, Std={predictions.std():.2f}, Range=[{predictions.min():.2f}, {predictions.max():.2f}]\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\" ✗ Error: {str(e)}\")\n",
    "\n",
    "        # BONUS: Create averaged predictions from top 3 models\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"BONUS: Creating Averaged Prediction from Top 3 Models\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        top_3 = sorted(results, key=lambda x: x['cv_rmse'])[:3]\n",
    "        print(\"\\nAveraging predictions from:\")\n",
    "        for i, m in enumerate(top_3, 1):\n",
    "            print(f\"  {i}. {m['name']} (CV RMSE: {m['cv_rmse']:.4f})\")\n",
    "\n",
    "        top_3_preds = [all_predictions[m['name']] for m in top_3 if m['name'] in all_predictions]\n",
    "\n",
    "        if len(top_3_preds) == 3:\n",
    "            averaged_preds = np.mean(top_3_preds, axis=0)\n",
    "\n",
    "            avg_submission = pd.DataFrame({\n",
    "                'Id': test_ids if test_ids is not None else range(len(averaged_preds)),\n",
    "                'Recovery Index': averaged_preds\n",
    "            })\n",
    "            avg_filename = f\"{output_folder}/TOP3_AVERAGED.csv\"\n",
    "            avg_submission.to_csv(avg_filename, index=False)\n",
    "\n",
    "            print(f\"\\n✓ Saved averaged predictions: {avg_filename}\")\n",
    "            print(f\"  Mean={averaged_preds.mean():.2f}, Std={averaged_preds.std():.2f}\")\n",
    "\n",
    "        # Summary\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nGenerated {len(all_predictions)} prediction files\")\n",
    "        print(f\"All files saved in: {output_folder}/\")\n",
    "        print(f\"\\nFiles created:\")\n",
    "\n",
    "        for i, (model_name, preds) in enumerate(all_predictions.items(), 1):\n",
    "            safe_name = model_name.replace(' ', '_').replace('(', '').replace(')', '').replace('=', '')\n",
    "            print(f\"   {i:2d}. {safe_name}.csv - {model_name}\")\n",
    "\n",
    "        print(f\"   {len(all_predictions)+1:2d}. TOP3_AVERAGED.csv - Average of top 3 models\")\n",
    "\n",
    "        print(\"\\nRECOMMENDATION:\")\n",
    "        print(f\"   - Start by submitting: {output_folder}/TOP3_AVERAGED.csv\")\n",
    "        print(f\"   - Then try: {top_3[0]['name'].replace(' ', '_').replace('(', '').replace(')', '').replace('=', '')}.csv\")\n",
    "        print(f\"   - Compare scores and pick the best!\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n[Step 5/5] No test file provided\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return results_df, results\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    TRAIN_PATH = 'train.csv'\n",
    "    TEST_PATH = 'test.csv'\n",
    "    OUTPUT_FOLDER = 'predictions_all_models'  # Folder to save all predictions\n",
    "\n",
    "    results_df, all_results = run_pipeline_all_models(\n",
    "        TRAIN_PATH, TEST_PATH, OUTPUT_FOLDER,\n",
    "        engineer_features=True, use_ensemble=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f8692c-a41f-415a-8a06-cbd4bd104e83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
